{
 "metadata": {
  "name": "",
  "signature": "sha256:34bfcd59d1081baab9e0e2cdf17329a98e7b5320c4da5dc4193fedede5cb191e"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Machine Learning"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Preamble\n",
      "\n",
      "- *Sami Niemi (s.niemi@ucl.ac.uk)*\n",
      "- *Data Analysis and Image Processing with Python, UCL/MSSL, November 4-5 2014*\n",
      "- *Notebook available on [github](https://github.com/sniemi/DataAnalysisPythonCourse)*\n",
      "\n",
      "This introduction to Machine Learning relies heavily on [Scikit-Learn](http://scikit-learn.org) and borrows several of their excellent examples. [Scikit-Learn](http://github.com/scikit-learn/scikit-learn) is a Python package designed to give access to machine learning algorithms within Python code.\n",
      "\n",
      "If you want to learn more beyound this short introduction, please take a look at the [Scikit-Learn Tutorial](http://scikit-learn.org/stable/tutorial/index.html)."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "What is Machine Learning?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The following text is from Wikipedia.\n",
      "    \n",
      "Machine learning is a scientific/engineering discipline that deals with the construction and study of algorithms that can learn from data. Such algorithms operate by **building a model based on inputs and using that to make predictions or decisions**, rather than following only explicitly programmed instructions.\n",
      "\n",
      "#### Supervised Learning\n",
      "\n",
      "Machine learning tasks can be of several forms. In **supervised learning**, the computer is presented with example inputs and their desired outputs, given by a \"teacher\", and the **goal is to learn a general rule that maps inputs to outputs**. Spam filtering is an example of supervised learning, in particular **classification**, where the learning algorithm is presented with email (or other) messages labeled beforehand as \"spam\" or \"not spam\", to produce a computer program that labels unseen messages as either spam or not.\n",
      "\n",
      "#### Unsupersived Learning\n",
      "\n",
      "In **unsupervised learning**, **no labels** are given to the learning algorithm, leaving it on its own to groups of similar inputs (**clustering**), **density estimates** or projections of high-dimensional data that can be visualised effectively. Unsupervised learning can be a goal in itself (discovering hidden patterns in data) or a means towards an end. Topic modeling is an example of unsupervised learning, where a program is given a list of human language documents and is tasked to find out which documents cover similar topics.\n",
      "\n",
      "\n",
      "#### In Summary \n",
      "\n",
      "Machine Learning is about creating models from data."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## How do I know which algorithm to choose?\n",
      "\n",
      "This flow chart by [Andreas Mueller](https://github.com/amueller) gives a nice summary of which algorithms to choose in various situations."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.display import Image\n",
      "Image(\"http://scikit-learn.org/dev/_static/ml_map.png\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## How should I format my data?\n",
      "\n",
      "Most if not all algorithms implemented in Scikit-Learn expect the data to be stored in a\n",
      "**2D NumPy array**. The size of the array should be `[n_samples, n_features]`. You can think\n",
      "about this as a table where each line is for example an observation and each column descibes\n",
      "a feature. In other words:\n",
      "\n",
      "- **n_samples:**   A sample can be an observation, object, image, video, or in general anything that can be describe with a number of features.\n",
      "- **n_features:**  In geneal, features can be any quantitative measure that can be described with numbers or booleans.\n",
      "\n",
      "There can be as many features as you can fit in the memory, but you must fix the number of features before you run a machine learning algorithm."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## How do I use Scikit-Learn algorithms?\n",
      "\n",
      "Algorithms are implemented with the following core methods:\n",
      "\n",
      "- fit = train an algorithm\n",
      "- predict = predict the value for a given record\n",
      "- predict_proba = predict the probability of all possible classes for a given record (classification only)\n",
      "- transform = alter your data based on a given preprocessor (i.e. normalize or scale your data) (preprocessing/unsuperivsed)\n",
      "- fit_transform = train a preprocessor and then transform the data in a single step (preprocessing/unsuperivsed)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Dimensional Reduction"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the following we use hand written digits. The aim is to classify them and then to predict if new hand written digits are given what they actually are. However, before we get to this predicting part we should visualise our data.\n",
      "\n",
      "Before trying to solve any particular problem it is always a good idea to visualise the data. In most real cases the data are multidimensional and at least I do not know a good way to visualise 20D data. Thus, it is a good idea to start by doing a \n",
      "**Dimensionality Reduction**. Here we apply the most straightforward one, namely **Principal Component Analysis** (PCA).\n",
      "\n",
      "Shortly, PCA uses an orthogonal transformation to **convert a set of observations** of possibly correlated variables **into a set of values of linearly uncorrelated variables** called principal components. This transformation is defined in such a way that **the first principal component has the largest possible variance** (that is, accounts for as much of the variability in the data as possible). The principal components are orthogonal because they are the eigenvectors of the covariance matrix, which is symmetric.\n",
      "\n",
      "Lets load the sklearn data and visualise it."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.datasets import load_digits\n",
      "from sklearn.decomposition import RandomizedPCA\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "digits = load_digits() #each image is 8x8 pixels\n",
      "\n",
      "fig = plt.figure(figsize=(6, 6))\n",
      "fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
      "\n",
      "for i in range(64):\n",
      "    ax = fig.add_subplot(8, 8, i + 1, xticks=[], yticks=[])\n",
      "    ax.imshow(digits.images[i], cmap=plt.cm.binary, interpolation='none')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Ok, looks pretty messy. What happens if we run PCA and look at the principle components:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pca = RandomizedPCA(n_components=2)\n",
      "proj = pca.fit_transform(digits.data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.scatter(proj[:, 0], proj[:, 1], c=digits.target)\n",
      "plt.colorbar()\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Some of the digits seem to cluster. However, for example 9s seem to be all over the place.\n",
      "\n",
      "However, PCA produces a linear mapping, which is fairly limited. If we want to try non-linear methods we can use the `manifold` module. Here we can try Multidimensional Scaling (MDS), which seeks a low-dimensional representation of the data in which the distances respect well the distances in the original high-dimensional space."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import manifold\n",
      "mds = manifold.MDS()\n",
      "Y = mds.fit_transform(digits.data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.scatter(Y[:, 0], Y[:, 1], c=digits.target)\n",
      "plt.colorbar()\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "That does seem to map the digits to separate clusters better than PCA. So, in principle we should be able to classify them with some success."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Gaussian Naive Bayes Classification: Handwritten Digits"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Lets look at a Scikit-Learn example where we try to predict handwritten digits.\n",
      "\n",
      "First, load data and visualise the data."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "digits = load_digits() #each image is 8x8 pixels\n",
      "fig = plt.figure(figsize=(6, 6))\n",
      "fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
      "\n",
      "for i in range(64):\n",
      "    ax = fig.add_subplot(8, 8, i + 1, xticks=[], yticks=[])\n",
      "    ax.imshow(digits.images[i], cmap=plt.cm.binary)\n",
      "    ax.text(0, 7, str(digits.target[i]))  #show the target value"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Lets apply Gaussian Naive Bayes method, which is a *generative* classifier. It fits an axis-aligned multi-dimensional Gaussian distribution to each training label, and uses this for a classification. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.naive_bayes import GaussianNB\n",
      "from sklearn.cross_validation import train_test_split\n",
      "\n",
      "X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target)\n",
      "\n",
      "clf = GaussianNB()\n",
      "clf.fit(X_train, y_train)\n",
      "\n",
      "predicted = clf.predict(X_test)\n",
      "expected = y_test"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "That was quick, lets look at the results."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig = plt.figure(figsize=(6, 6)) \n",
      "\n",
      "for i in range(64):\n",
      "    ax = fig.add_subplot(8, 8, i + 1, xticks=[], yticks=[])\n",
      "    ax.imshow(X_test.reshape(-1, 8, 8)[i], cmap=plt.cm.binary, interpolation='nearest')\n",
      "    \n",
      "    if predicted[i] == expected[i]:\n",
      "        ax.text(0, 7, str(predicted[i]), color='green')\n",
      "    else:\n",
      "        ax.text(0, 7, str(predicted[i]), color='red')\n",
      "        ax.text(0, 2, str(expected[i]), color='blue')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Not bad for a quick and dirty. How well, did we do?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import metrics\n",
      "print metrics.classification_report(expected, predicted)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "It looks like we did not do very well with 1s, 3s, 7s an 8s. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print metrics.confusion_matrix(expected, predicted)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "It looks like that we confuse 1s, 2s, 3s, and 9s with 8s."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Clustering"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The following example is from http://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_comparison.html#example-cluster-plot-cluster-comparison-py"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "from sklearn import cluster, datasets\n",
      "from sklearn.metrics import euclidean_distances\n",
      "from sklearn.neighbors import kneighbors_graph\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "np.random.seed(0)\n",
      "\n",
      "# Generate datasets. We choose the size big enough to see the scalability\n",
      "# of the algorithms, but not too big to avoid too long running times\n",
      "n_samples = 1500\n",
      "noisy_circles = datasets.make_circles(n_samples=n_samples, factor=.5,\n",
      "                                      noise=.05)\n",
      "noisy_moons = datasets.make_moons(n_samples=n_samples, noise=.05)\n",
      "blobs = datasets.make_blobs(n_samples=n_samples, random_state=8)\n",
      "no_structure = np.random.rand(n_samples, 2), None\n",
      "\n",
      "colors = np.array([x for x in 'bgrcmykbgrcmykbgrcmykbgrcmyk'])\n",
      "colors = np.hstack([colors] * 20)\n",
      "\n",
      "plt.figure(figsize=(17, 9.5))\n",
      "plt.subplots_adjust(left=.001, right=.999, bottom=.001, top=.96, wspace=.05,\n",
      "                    hspace=.01)\n",
      "\n",
      "plot_num = 1\n",
      "for i_dataset, dataset in enumerate([noisy_circles, noisy_moons, blobs,\n",
      "                                     no_structure]):\n",
      "    X, y = dataset\n",
      "    # normalize dataset for easier parameter selection\n",
      "    X = StandardScaler().fit_transform(X)\n",
      "\n",
      "    # estimate bandwidth for mean shift\n",
      "    bandwidth = cluster.estimate_bandwidth(X, quantile=0.3)\n",
      "\n",
      "    # connectivity matrix for structured Ward\n",
      "    connectivity = kneighbors_graph(X, n_neighbors=10)\n",
      "    # make connectivity symmetric\n",
      "    connectivity = 0.5 * (connectivity + connectivity.T)\n",
      "\n",
      "    # Compute distances\n",
      "    #distances = np.exp(-euclidean_distances(X))\n",
      "    distances = euclidean_distances(X)\n",
      "\n",
      "    # create clustering estimators\n",
      "    ms = cluster.MeanShift(bandwidth=bandwidth, bin_seeding=True)\n",
      "    two_means = cluster.MiniBatchKMeans(n_clusters=2)\n",
      "    ward = cluster.AgglomerativeClustering(n_clusters=2,\n",
      "                    linkage='ward', connectivity=connectivity)\n",
      "    spectral = cluster.SpectralClustering(n_clusters=2,\n",
      "                                          eigen_solver='arpack',\n",
      "                                          affinity=\"nearest_neighbors\")\n",
      "    dbscan = cluster.DBSCAN(eps=.2)\n",
      "    affinity_propagation = cluster.AffinityPropagation(damping=.9,\n",
      "                                                       preference=-200)\n",
      "\n",
      "    average_linkage = cluster.AgglomerativeClustering(linkage=\"average\",\n",
      "                            affinity=\"cityblock\", n_clusters=2,\n",
      "                            connectivity=connectivity)\n",
      "\n",
      "    for name, algorithm in [\n",
      "                            ('MiniBatchKMeans', two_means),\n",
      "                            ('AffinityPropagation', affinity_propagation),\n",
      "                            ('MeanShift', ms),\n",
      "                            ('SpectralClustering', spectral),\n",
      "                            ('Ward', ward),\n",
      "                            ('AgglomerativeClustering', average_linkage),\n",
      "                            ('DBSCAN', dbscan)\n",
      "                           ]:\n",
      "        # predict cluster memberships\n",
      "        t0 = time.time()\n",
      "        algorithm.fit(X)\n",
      "        t1 = time.time()\n",
      "        if hasattr(algorithm, 'labels_'):\n",
      "            y_pred = algorithm.labels_.astype(np.int)\n",
      "        else:\n",
      "            y_pred = algorithm.predict(X)\n",
      "\n",
      "        # plot\n",
      "        plt.subplot(4, 7, plot_num)\n",
      "        if i_dataset == 0:\n",
      "            plt.title(name, size=18)\n",
      "        plt.scatter(X[:, 0], X[:, 1], color=colors[y_pred].tolist(), s=10)\n",
      "\n",
      "        if hasattr(algorithm, 'cluster_centers_'):\n",
      "            centers = algorithm.cluster_centers_\n",
      "            center_colors = colors[:len(centers)]\n",
      "            plt.scatter(centers[:, 0], centers[:, 1], s=100, c=center_colors)\n",
      "        plt.xlim(-2, 2)\n",
      "        plt.ylim(-2, 2)\n",
      "        plt.xticks(())\n",
      "        plt.yticks(())\n",
      "        plt.text(.99, .01, ('%.2fs' % (t1 - t0)).lstrip('0'),\n",
      "                 transform=plt.gca().transAxes, size=15,\n",
      "                 horizontalalignment='right')\n",
      "        plot_num += 1\n",
      "\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Regression"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}