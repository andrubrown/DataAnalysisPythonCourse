{
 "metadata": {
  "name": "",
  "signature": "sha256:99b0e082c7e5dcc1b00678374e9c94703a12a51b6c23173703124d55e26e9953"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Machine Learning"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Preamble\n",
      "\n",
      "- *Sami Niemi (s.niemi@ucl.ac.uk)*\n",
      "- *Data Analysis and Image Processing with Python, UCL/MSSL, November 4-5 2014*\n",
      "- *Notebook available on [github](https://github.com/sniemi/DataAnalysisPythonCourse)*\n",
      "\n",
      "This introduction to Machine Learning relies heavily on [Scikit-Learn](http://scikit-learn.org) and borrows several of their excellent examples. [Scikit-Learn](http://github.com/scikit-learn/scikit-learn) is a Python package designed to give access to machine learning algorithms within Python code.\n",
      "\n",
      "If you want to learn more beyound this short introduction, please take a look at the [Scikit-Learn Tutorial](http://scikit-learn.org/stable/tutorial/index.html)."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "What is Machine Learning?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The following text is from Wikipedia.\n",
      "    \n",
      "Machine learning is a scientific/engineering discipline that deals with the construction and study of algorithms that can learn from data. Such algorithms operate by **building a model based on inputs and using that to make predictions or decisions**, rather than following only explicitly programmed instructions.\n",
      "\n",
      "#### Supervised Learning\n",
      "\n",
      "Machine learning tasks can be of several forms. In **supervised learning**, the computer is presented with example inputs and their desired outputs, given by a \"teacher\", and the **goal is to learn a general rule that maps inputs to outputs**. Spam filtering is an example of supervised learning, in particular **classification**, where the learning algorithm is presented with email (or other) messages labeled beforehand as \"spam\" or \"not spam\", to produce a computer program that labels unseen messages as either spam or not.\n",
      "\n",
      "#### Unsupersived Learning\n",
      "\n",
      "In **unsupervised learning**, **no labels** are given to the learning algorithm, leaving it on its own to groups of similar inputs (**clustering**), **density estimates** or projections of high-dimensional data that can be visualised effectively. Unsupervised learning can be a goal in itself (discovering hidden patterns in data) or a means towards an end. Topic modeling is an example of unsupervised learning, where a program is given a list of human language documents and is tasked to find out which documents cover similar topics.\n",
      "\n",
      "\n",
      "#### In Summary \n",
      "\n",
      "Machine Learning is about creating models from data."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## How do I know which algorithm to choose?\n",
      "\n",
      "This flow chart by [Andreas Mueller](https://github.com/amueller) gives a nice summary of which algorithms to choose in various situations."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.display import Image\n",
      "Image(\"http://scikit-learn.org/dev/_static/ml_map.png\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## How should I format my data?\n",
      "\n",
      "Most if not all algorithms implemented in Scikit-Learn expect the data to be stored in a\n",
      "**2D NumPy array**. The size of the array should be `[n_samples, n_features]`. You can think\n",
      "about this as a table where each line is for example an observation and each column descibes\n",
      "a feature. In other words:\n",
      "\n",
      "- **n_samples:**   A sample can be an observation, object, image, video, or in general anything that can be describe with a number of features.\n",
      "- **n_features:**  In geneal, features can be any quantitative measure that can be described with numbers or booleans.\n",
      "\n",
      "There can be as many features as you can fit in the memory, but you must fix the number of features before you run a machine learning algorithm."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## How do I use Scikit-Learn algorithms?\n",
      "\n",
      "Algorithms are implemented with the following core methods:\n",
      "\n",
      "- fit = train an algorithm\n",
      "- predict = predict the value for a given record\n",
      "- predict_proba = predict the probability of all possible classes for a given record (classification only)\n",
      "- transform = alter your data based on a given preprocessor (i.e. normalize or scale your data) (preprocessing/unsuperivsed)\n",
      "- fit_transform = train a preprocessor and then transform the data in a single step (preprocessing/unsuperivsed)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Dimensional Reduction"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Reducing the number of random variables to consider.**\n",
      "\n",
      "**Applications**: Image and spectral decomposition and reconstruction, visualisation, Increased efficiency..."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the following we use hand written digits. The aim is to classify them and then to predict if new hand written digits are given what they actually are. However, before we get to this predicting part we should visualise our data.\n",
      "\n",
      "Before trying to solve any particular problem it is always a good idea to visualise the data. In most real cases the data are multidimensional and at least I do not know a good way to visualise 20D data. Thus, it is a good idea to start by doing a \n",
      "**Dimensionality Reduction**. Here we apply the most straightforward one, namely **Principal Component Analysis** (PCA).\n",
      "\n",
      "Shortly, PCA uses an orthogonal transformation to **convert a set of observations** of possibly correlated variables **into a set of values of linearly uncorrelated variables** called principal components. This transformation is defined in such a way that **the first principal component has the largest possible variance** (that is, accounts for as much of the variability in the data as possible). The principal components are orthogonal because they are the eigenvectors of the covariance matrix, which is symmetric.\n",
      "\n",
      "Lets load the sklearn data and visualise it."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.datasets import load_digits\n",
      "from sklearn.decomposition import RandomizedPCA\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "digits = load_digits() #each image is 8x8 pixels\n",
      "print digits['images'].shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'Lets plot 128 out of the 1797:'\n",
      "fig = plt.figure(figsize=(12, 12))\n",
      "fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
      "\n",
      "for i in range(128):\n",
      "    ax = fig.add_subplot(16, 16, i + 1, xticks=[], yticks=[])\n",
      "    ax.imshow(digits.images[i], cmap=plt.cm.binary, interpolation='none')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Ok, looks pretty messy. What happens if we run PCA and look at the principle components:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pca = RandomizedPCA(n_components=2, whiten=True)\n",
      "proj = pca.fit_transform(digits.data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.scatter(proj[:, 0], proj[:, 1], c=digits.target)\n",
      "plt.colorbar()\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Some of the digits seem to cluster. However, for example 9s seem to be all over the place.\n",
      "\n",
      "However, PCA produces a linear mapping, which is fairly limited. If we want to try non-linear methods we can use the `manifold` module. Here we can try Multidimensional Scaling (MDS), which seeks a low-dimensional representation of the data in which the distances respect well the distances in the original high-dimensional space."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import manifold\n",
      "mds = manifold.MDS(max_iter=500)\n",
      "Y = mds.fit_transform(digits.data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.scatter(Y[:, 0], Y[:, 1], c=digits.target)\n",
      "plt.colorbar()\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "That does seem to map the digits to separate clusters better than PCA. So, in principle we should be able to classify them with some success."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Classification\n",
      "\n",
      "**Identifying to which set of categories a new observation belong to.**\n",
      "\n",
      "**Applications**: Quasar-star separation, spam detection, image recognition..."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Gaussian Naive Bayes Classification: Handwritten Digits"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Lets look at a Scikit-Learn example where we try to predict handwritten digits.\n",
      "\n",
      "First, load data and visualise the data."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig = plt.figure(figsize=(12, 12))\n",
      "fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
      "\n",
      "for i in range(128):\n",
      "    ax = fig.add_subplot(16, 16, i + 1, xticks=[], yticks=[])\n",
      "    ax.imshow(digits.images[i], cmap=plt.cm.binary)\n",
      "    ax.text(0, 7, str(digits.target[i]), color='g')  #show the target value"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Lets apply Gaussian Naive Bayes method, which is a *generative* classifier. It fits an axis-aligned multi-dimensional Gaussian distribution to each training label, and uses this for a classification. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.naive_bayes import GaussianNB\n",
      "from sklearn.cross_validation import train_test_split\n",
      "\n",
      "digits = load_digits()\n",
      "X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target)\n",
      "\n",
      "clf = GaussianNB()\n",
      "clf.fit(X_train, y_train)\n",
      "\n",
      "predicted = clf.predict(X_test)\n",
      "expected = y_test"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "That was quick, lets look at the results."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig = plt.figure(figsize=(14, 18)) \n",
      "\n",
      "for i in range(128):\n",
      "    ax = fig.add_subplot(16, 16, i + 1, xticks=[], yticks=[])\n",
      "    ax.imshow(X_test.reshape(-1, 8, 8)[i], cmap=plt.cm.binary, interpolation='nearest')\n",
      "    \n",
      "    if predicted[i] == expected[i]:\n",
      "        ax.text(0, 7, str(predicted[i]), color='green')\n",
      "    else:\n",
      "        ax.text(0, 7, str(predicted[i]), color='red')\n",
      "        ax.text(0, 2, str(expected[i]), color='blue')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Not bad for a quick and dirty. How well, did we do?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import metrics\n",
      "print metrics.classification_report(expected, predicted)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "It looks like we did not do very well with 1s, 3s, 7s an 8s. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print metrics.confusion_matrix(expected, predicted)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "It looks like that we confuse 1s, 2s, 3s, and 9s with 8s.\n",
      "\n",
      "Can we do better? We could try using e.g. Support Vector Machines or Decision trees..."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Random Forest Classification: Handwritten Digits."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.ensemble import RandomForestClassifier\n",
      "\n",
      "digits = load_digits()\n",
      "X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target)\n",
      "\n",
      "forest = RandomForestClassifier(n_estimators=100, max_depth=50, verbose=True)\n",
      "forest.fit(X_train, y_train)\n",
      "\n",
      "predictedF = clf.predict(X_test)\n",
      "expectedF = y_test"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Lets check the results"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig = plt.figure(figsize=(14, 18)) \n",
      "\n",
      "for i in range(128):\n",
      "    ax = fig.add_subplot(16, 16, i + 1, xticks=[], yticks=[])\n",
      "    ax.imshow(X_test.reshape(-1, 8, 8)[i], cmap=plt.cm.binary, interpolation='nearest')\n",
      "    \n",
      "    if predictedF[i] == expectedF[i]:\n",
      "        ax.text(0, 7, str(predictedF[i]), color='green')\n",
      "    else:\n",
      "        ax.text(0, 7, str(predictedF[i]), color='red')\n",
      "        ax.text(0, 2, str(expectedF[i]), color='blue')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print metrics.classification_report(expectedF, predictedF)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import metrics\n",
      "\n",
      "plt.figure(figsize=(8,8))\n",
      "plt.imshow(metrics.confusion_matrix(predictedF, y_test),\n",
      "           interpolation='nearest', cmap=plt.cm.binary)\n",
      "plt.grid(False)\n",
      "plt.colorbar()\n",
      "plt.xlabel(\"true\")\n",
      "plt.ylabel(\"predicted\")\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Clustering\n",
      "\n",
      "**Automatic grouping of similar objects into sets.**\n",
      "\n",
      "**Applications**: Astronomical structure finding, customer segmentation, grouping experiment outcomes..."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In clustering we are trying to find groups  in data based on the data not the labels.\n",
      "\n",
      "In the following we first look at a very simple example, and then quickly different clustering methods. We start by looking K Means. K Means searches for cluster centers which are the mean of the points within them, such that every point is closest to the cluster center it is assigned to.\n",
      "\n",
      "Lets start by generating some fake data that has 10 clusters."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.datasets.samples_generator import make_blobs\n",
      "clusters = 10\n",
      "features = 2\n",
      "X, y = make_blobs(n_samples=1000, n_features=features, centers=clusters, random_state=0, cluster_std=0.8)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.figure(figsize=(8, 6))\n",
      "plt.scatter(X[:, 0], X[:, 1], s=30)\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.cluster import KMeans\n",
      "est = KMeans(clusters)\n",
      "est.fit(X)\n",
      "y_kmeans = est.predict(X)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.figure(figsize=(8, 6))\n",
      "plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=20)\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Seems to work pretty well, how about if you change cluster_std? Try doing the same with more features..."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Different Clustering Methods"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The following example is from http://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_comparison.html#example-cluster-plot-cluster-comparison-py"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "from sklearn import cluster, datasets\n",
      "from sklearn.metrics import euclidean_distances\n",
      "from sklearn.neighbors import kneighbors_graph\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "np.random.seed(0)\n",
      "\n",
      "# Generate datasets. We choose the size big enough to see the scalability\n",
      "# of the algorithms, but not too big to avoid too long running times\n",
      "n_samples = 1500\n",
      "noisy_circles = datasets.make_circles(n_samples=n_samples, factor=.5,\n",
      "                                      noise=.05)\n",
      "noisy_moons = datasets.make_moons(n_samples=n_samples, noise=.05)\n",
      "blobs = datasets.make_blobs(n_samples=n_samples, random_state=8)\n",
      "no_structure = np.random.rand(n_samples, 2), None"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "colors = np.array([x for x in 'bgrcmykbgrcmykbgrcmykbgrcmyk'])\n",
      "colors = np.hstack([colors] * 20)\n",
      "\n",
      "plt.figure(figsize=(17, 9.5))\n",
      "plt.subplots_adjust(left=.001, right=.999, bottom=.001, top=.96, wspace=.05, hspace=.01)\n",
      "\n",
      "plot_num = 1\n",
      "for i_dataset, dataset in enumerate([noisy_circles, noisy_moons, blobs, no_structure]):\n",
      "    X, y = dataset\n",
      "    # normalize dataset for easier parameter selection\n",
      "    X = StandardScaler().fit_transform(X)\n",
      "\n",
      "    # estimate bandwidth for mean shift\n",
      "    bandwidth = cluster.estimate_bandwidth(X, quantile=0.3)\n",
      "\n",
      "    # connectivity matrix for structured Ward\n",
      "    connectivity = kneighbors_graph(X, n_neighbors=10)\n",
      "    # make connectivity symmetric\n",
      "    connectivity = 0.5 * (connectivity + connectivity.T)\n",
      "\n",
      "    # Compute distances\n",
      "    #distances = np.exp(-euclidean_distances(X))\n",
      "    distances = euclidean_distances(X)\n",
      "\n",
      "    # create clustering estimators\n",
      "    ms = cluster.MeanShift(bandwidth=bandwidth, bin_seeding=True)\n",
      "    two_means = cluster.MiniBatchKMeans(n_clusters=2)\n",
      "    ward = cluster.AgglomerativeClustering(n_clusters=2,\n",
      "                    linkage='ward', connectivity=connectivity)\n",
      "    spectral = cluster.SpectralClustering(n_clusters=2,\n",
      "                                          eigen_solver='arpack',\n",
      "                                          affinity=\"nearest_neighbors\")\n",
      "    dbscan = cluster.DBSCAN(eps=.2)\n",
      "    affinity_propagation = cluster.AffinityPropagation(damping=.9,\n",
      "                                                       preference=-200)\n",
      "\n",
      "    average_linkage = cluster.AgglomerativeClustering(linkage=\"average\",\n",
      "                            affinity=\"cityblock\", n_clusters=2,\n",
      "                            connectivity=connectivity)\n",
      "\n",
      "    for name, algorithm in [\n",
      "                            ('MiniBatchKMeans', two_means),\n",
      "                            ('AffinityPropagation', affinity_propagation),\n",
      "                            ('MeanShift', ms),\n",
      "                            ('SpectralClustering', spectral),\n",
      "                            ('Ward', ward),\n",
      "                            ('AgglomerativeClustering', average_linkage),\n",
      "                            ('DBSCAN', dbscan)\n",
      "                           ]:\n",
      "        # predict cluster memberships\n",
      "        t0 = time.time()\n",
      "        algorithm.fit(X)\n",
      "        t1 = time.time()\n",
      "        if hasattr(algorithm, 'labels_'):\n",
      "            y_pred = algorithm.labels_.astype(np.int)\n",
      "        else:\n",
      "            y_pred = algorithm.predict(X)\n",
      "\n",
      "        # plot\n",
      "        plt.subplot(4, 7, plot_num)\n",
      "        if i_dataset == 0:\n",
      "            plt.title(name, size=18)\n",
      "        plt.scatter(X[:, 0], X[:, 1], color=colors[y_pred].tolist(), s=10)\n",
      "\n",
      "        if hasattr(algorithm, 'cluster_centers_'):\n",
      "            centers = algorithm.cluster_centers_\n",
      "            center_colors = colors[:len(centers)]\n",
      "            plt.scatter(centers[:, 0], centers[:, 1], s=100, c=center_colors)\n",
      "        plt.xlim(-2, 2)\n",
      "        plt.ylim(-2, 2)\n",
      "        plt.xticks(())\n",
      "        plt.yticks(())\n",
      "        plt.text(.99, .01, ('%.2fs' % (t1 - t0)).lstrip('0'),\n",
      "                 transform=plt.gca().transAxes, size=15,\n",
      "                 horizontalalignment='right')\n",
      "        plot_num += 1\n",
      "\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "DBSCAN seems to do a good job, but there is no guarantee that it will work well in your particular problem..."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Regression"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Predicting a continuous value for a new datum.**\n",
      "\n",
      "**Applications**: Photometric redshift, expansion of the Universe, drug response, stock prices..."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Gaussian Process\n",
      "\n",
      "From Wikipedia:  Gaussian process is a stochastic process whose realizations consist of random values associated with every point in a range of times (or of space) such that each such random variable has a normal distribution.\n",
      "\n",
      "Wait, what?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Lets look at a toy example. You can think about this as a light curve and ask: if we have noisy observations of a dependent variable, think flux, at certain times, what is our best estimate for the flux at some new time?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def f(x):\n",
      "    'the \\\"lightcurve\\\"'\n",
      "    return 0.9*x * np.cos(x) + 20"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'Generate data and add noise'\n",
      "np.random.seed(10)\n",
      "X = np.linspace(0., 11.5, 25)\n",
      "X = np.atleast_2d(X).T\n",
      "y = f(X).ravel()\n",
      "dy = 0.4 + 1.7 * np.random.random(y.shape)\n",
      "noise = np.random.normal(0, dy)\n",
      "y += noise"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'Plot the \\\"lightcurve\\\":' \n",
      "fig = plt.figure(figsize=(12,8))\n",
      "plt.errorbar(X.ravel(), y, dy, fmt='r.', markersize=15, label='Observations')\n",
      "plt.plot(x, f(x), 'r--', label=u'$f(x) = 0.9x\\,\\cos(x)$', lw=4, alpha=0.1)\n",
      "plt.errorbar([12.5,], f(12.5), [5,], fmt='gs', markersize=9, label='New point?')\n",
      "plt.ylim(0, 45)\n",
      "plt.xlim(-1, 13)\n",
      "plt.xlabel('$\\mathrm{time}\\quad [s]$', size=20)\n",
      "plt.ylabel('$\\mathrm{Intensity} \\quad f(x)$', size=20)\n",
      "plt.legend(numpoints=1, fancybox=True, shadow=True, loc='best')\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.gaussian_process import GaussianProcess\n",
      "\n",
      "# generate grid on which to evaluate the GP\n",
      "x = np.atleast_2d(np.linspace(-1, 13, 1000)).T\n",
      "\n",
      "gp = GaussianProcess(regr='quadratic', corr='squared_exponential', theta0=1e-1, thetaL=1e-4, thetaU=1,\n",
      "                     nugget=(dy/y)**2, random_start=1000, verbose=True)\n",
      "gp.fit(X, y) # Fit to data using Maximum Likelihood Estimation of the parameters\n",
      "\n",
      "# Make the prediction on the grid and Mean Squared Error\n",
      "y_pred, MSE = gp.predict(x, eval_MSE=True)\n",
      "sigma = np.sqrt(MSE)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'Plot the \\\"lightcurve\\\" and include our prediction:'\n",
      "fig = plt.figure(figsize=(12,8))\n",
      "plt.plot(x, f(x), 'r--', label=u'$f(x) = 0.9x\\,\\cos(x)$', lw=4)\n",
      "plt.errorbar(X.ravel(), y, dy, fmt='r.', markersize=15, label='Observations')\n",
      "plt.errorbar([12.5,], f(12.5), [5,], fmt='gs', markersize=9, label='New point?')\n",
      "plt.plot(x, y_pred, 'b-', label=u'Prediction', lw=2)\n",
      "plt.fill(np.concatenate([x, x[::-1]]),\n",
      "         np.concatenate([y_pred - 1.9600 * sigma,\n",
      "                        (y_pred + 1.9600 * sigma)[::-1]]),\n",
      "         alpha=.3, fc='b', ec='None', label=u'95\\% confidence interval')\n",
      "plt.xlabel('$\\mathrm{time}\\quad [s]$', size=20)\n",
      "plt.ylabel('$\\mathrm{Intensity} \\quad f(x)$', size=20)\n",
      "plt.ylim(0, 45)\n",
      "plt.xlim(-1, 13)\n",
      "plt.legend(numpoints=1, fancybox=True, shadow=True, loc='best')\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Our prediction based on Gaussian Process is pretty close to the true point and is clearly within the 95 percent confidence internval."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For futher reading, please see:\n",
      "- http://scikit-learn.org/stable/modules/gaussian_process.html\n",
      "- http://mlg.eng.cam.ac.uk/zoubin/tut06/snelson-gp.pdf"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}